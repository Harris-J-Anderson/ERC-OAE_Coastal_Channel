{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34330ceb-a070-493e-9b7b-b35dfe98020a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client,LocalCluster\n",
    "from dask_jobqueue import PBSCluster\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.markers import MarkerStyle\n",
    "import cmocean\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "import dask as da\n",
    "import geopandas as gpd\n",
    "import cartopy as cart\n",
    "import cartopy.crs as ccrs\n",
    "import pandas as pd\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import emsarray as emr\n",
    "import emsarray\n",
    "import calendar\n",
    "from shapely.geometry import Point, Polygon, box, shape\n",
    "from alphashape import alphashape\n",
    "import shapely\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import time\n",
    "import regionmask\n",
    "import re as re\n",
    "from windrose import WindroseAxes\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import glob\n",
    "import PyCO2SYS as pyco2\n",
    "from IPython.display import display, HTML\n",
    "import great_circle_calculator.great_circle_calculator as gcc\n",
    "import gsw as sw\n",
    "\n",
    "import math as math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2253bf16-69e8-43a4-ba28-a53eea2078bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10, 6]   # Default figure size\n",
    "plt.rcParams['figure.dpi'] = 100           # Default DPI\n",
    "plt.rcParams['savefig.dpi'] = 300          # DPI for saving figures\n",
    "plt.rcParams['font.size'] = 12             # Default font size\n",
    "plt.rcParams['axes.titlesize'] = 14        # Title font size\n",
    "plt.rcParams['axes.labelsize'] = 12        # Axis label font size\n",
    "plt.rcParams['xtick.labelsize'] = 11       # X-tick label font size\n",
    "plt.rcParams['ytick.labelsize'] = 11       # Y-tick label font size\n",
    "plt.rcParams['legend.fontsize'] = 10       # Legend font size\n",
    "plt.rcParams['lines.linewidth'] = 1.5      # Line width\n",
    "plt.rcParams['lines.markersize'] = 8       # Marker size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90830525-0a51-4610-b020-e820e344a459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_div(numerator,denominator):\n",
    "    if denominator != 0:\n",
    "        return numerator/denominator\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def read_shoc_nc(path,ems=True):\n",
    "  \n",
    "    files = glob.glob(path)\n",
    "\n",
    "    files.sort()\n",
    "\n",
    "    if ems == True:\n",
    "\n",
    "        ds = emsarray.accessors.xarray.open_mfdataset(files,concat_dim='time',combine='nested',\n",
    "                           data_vars='minimal',compat='override',\n",
    "                           coords='minimal',parallel='true')\n",
    "\n",
    "    else:\n",
    "\n",
    "        ds = xr.open_mfdataset(files,concat_dim='time',combine='nested',\n",
    "                               data_vars='minimal',compat='override',\n",
    "                               coords='minimal',parallel='true')\n",
    "\n",
    "    return ds\n",
    "\n",
    "def extent(data):\n",
    "\n",
    "    if 'longitude' in data.coords:\n",
    "        \n",
    "        extent = (np.nanmin(data['longitude'].values)-0.005,np.nanmax(data['longitude'].values)+0.005,\n",
    "                  np.nanmin(data['latitude'].values)-0.005,np.nanmax(data['latitude'].values)+0.005)\n",
    "        \n",
    "    else:\n",
    "\n",
    "        extent = (np.nanmin(data['x_centre'].values)-0.005,np.nanmax(data['x_centre'].values)+0.005,\n",
    "                  np.nanmin(data['y_centre'].values)-0.005,np.nanmax(data['y_centre'].values)+0.005)\n",
    "        \n",
    "    return extent\n",
    "\n",
    "def running_mean(data, window_size):\n",
    "    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71d18ee2-2e84-48c0-adf4-a0d643ed11c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os,sys\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from datetime import date\n",
    "\n",
    "def shoc_days(years_since,year,month,day,hours=None,minutes=None,seconds=None):\n",
    "\n",
    "    hours = hours or 0\n",
    "    minutes = minutes or 0\n",
    "    seconds = seconds or 0\n",
    "\n",
    "    # Calculate the difference in days between the end date and the begin date\n",
    "    dates = datetime(year,month,day,hours,minutes,seconds)\n",
    "\n",
    "    day_one = datetime(years_since,1,1)\n",
    "    \n",
    "    time_difference = (dates - day_one)\n",
    "\n",
    "    days_with_fraction = time_difference.days + (time_difference.seconds / 86400)\n",
    "    \n",
    "    return days_with_fraction\n",
    "\n",
    "def shoc_date(start_yr,days_in): # Input days as float of days since first day, i.e., 4718.236\n",
    "\n",
    "    days_since_string = str(start_yr)+\"-1-1\"\n",
    "    day_one = datetime.strptime(days_since_string,\"%Y-%m-%d\")\n",
    "    end_date = day_one + timedelta(days=days_in)\n",
    "    \n",
    "    return end_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784d8763-954a-4a38-8cb1-8fdec0c48577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_ts(ts_file_path): \n",
    "    \n",
    "    ts_file = str(ts_file_path)\n",
    "    \n",
    "    import re\n",
    "    \n",
    "    comment = '#'\n",
    "    columns = []\n",
    "    \n",
    "    with open(ts_file,'r') as td:\n",
    "        for line in td:\n",
    "            line = line.rstrip()\n",
    "            if line[0] == comment:\n",
    "                if '.name' in line: \n",
    "                    inf = re.split(' +',line)\n",
    "                    columns.append(inf[2])\n",
    "    \n",
    "            else:\n",
    "                _dfs = [\n",
    "                    pd.DataFrame([line.split(' ')],columns=columns, dtype=float),\n",
    "                    pd.read_table(td,sep=' ', header=None, names=columns)]\n",
    "                df = pd.concat(_dfs,ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a206b423-5682-4f28-b3c2-56bd5f78c846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_cluser(cores,hours=None,mins=None):\n",
    "\n",
    "    hours = hours or '00'\n",
    "    if not isinstance(hours, str):\n",
    "        if float(hours) < 10:\n",
    "            hours = '0' + str(hours)\n",
    "\n",
    "    mins = mins or '00'\n",
    "    if not isinstance(mins, str):\n",
    "        if float(mins) < 10:\n",
    "            mins = '0' + str(mins)\n",
    "        \n",
    "    walltime = str(hours) + ':' + str(mins) + ':00'\n",
    "    cores = cores\n",
    "    memory = str(4 * cores) + 'GB'\n",
    "    \n",
    "    cluster = PBSCluster(walltime=str(walltime), cores=cores, memory=str(memory), processes=cores,\n",
    "                         job_extra_directives=['-q normalsr',\n",
    "                                               '-P ih54',\n",
    "                                               '-l ncpus='+str(cores),\n",
    "                                               '-l mem='+str(memory),\n",
    "                                               '-l storage=scratch/et4+gdata/et4+gdata/ew0+gdata/ih54'],\n",
    "                         local_directory='$TMPDIR',\n",
    "                         job_directives_skip=[\"select\"])\n",
    "                         # python=os.environ[\"DASK_PYTHON\"])\n",
    "\n",
    "    return cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ae16af-8d02-45d6-9de5-61bd8128de71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plume_size(xr_ds,input,threshold,var):\n",
    "\n",
    "    cell_area = (input.h1acell * input.h2acell).values\n",
    "\n",
    "    plume = (xr_ds[var].isel(k=18) > threshold).values\n",
    "\n",
    "    binary_plume = plume.astype(int)\n",
    "\n",
    "    square_km = np.zeros((len(xr_ds.time)))\n",
    "\n",
    "    for i in tqdm_notebook(range(len(xr_ds.time)),desc='progress...'):\n",
    "\n",
    "        data = cell_area * binary_plume[i,:,:]\n",
    "    \n",
    "        square_km[i] = np.nansum(data/1e6) \n",
    "        \n",
    "    return square_km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcbd1755-0d97-49ca-a906-3fef080e95ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def marine_res_mask(gpd_df, template_ds, boundary=None, lon_name='x', lat_name='y'):\n",
    "    \n",
    "    mask = regionmask.mask_3D_geopandas(\n",
    "        gpd_df,\n",
    "        template_ds[lon_name],\n",
    "        template_ds[lat_name]\n",
    "    )\n",
    "    \n",
    "    if lon_name != 'lon':\n",
    "        mask = mask.rename({lon_name: 'lon'})\n",
    "    if lat_name != 'lat':\n",
    "        mask = mask.rename({lat_name: 'lat'})\n",
    "        \n",
    "    if isinstance(boundary, list):\n",
    "        mask = mask.sel(\n",
    "            lon=slice(boundary[0], boundary[1]),\n",
    "            lat=slice(boundary[2], boundary[3])\n",
    "        )\n",
    "        \n",
    "    return mask\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6779b798-37dd-40ea-b2ac-44d07d5823da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shoc_ssh_mask(dataset,original_in_file,var,depth_slice=False,depth=None):\n",
    "\n",
    "    depth_mask = np.empty((len(dataset.time),len(dataset.j),len(dataset.i)))\n",
    "    \n",
    "    new_var = np.zeros((len(dataset.time),len(dataset.k),len(dataset.j),len(dataset.i)))\n",
    "    \n",
    "    if depth_slice:\n",
    "        \n",
    "        depth_slice_mask = np.zeros((len(dataset.time),len(dataset.j),len(dataset.i)))\n",
    "\n",
    "        depth_slice_var = np.zeros((len(dataset.time),len(dataset.k),len(dataset.j),len(dataset.i)))\n",
    "    \n",
    "    depths = original_in_file.z_grid.values\n",
    "    eta_vals = dataset.eta.values\n",
    "    var_vals = dataset[var].values\n",
    "    \n",
    "    dims=['time','k','j','i']\n",
    "    \n",
    "    for t in tqdm_notebook(range(len(dataset.time)),desc=\"progress\"):\n",
    "    \n",
    "        eta = eta_vals[t,:,:]\n",
    "        variable = var_vals[t,:,:,:]\n",
    "    \n",
    "        for j in range(len(dataset.j)):\n",
    "            \n",
    "            for i in range(len(dataset.i)):\n",
    "        \n",
    "                depth_mask[t,j,i] = np.abs(depths-eta[j,i]).argmin() \n",
    "\n",
    "                if depth_slice:\n",
    "\n",
    "                    depth_slice_mask[t,j,i] = np.abs(depths-(eta[j,i]-depth)).argmin()\n",
    "        \n",
    "        for k in range(len(dataset.k)):\n",
    "            \n",
    "            for j in range(len(dataset.j)):\n",
    "                \n",
    "                for i in range(len(dataset.i)):\n",
    "\n",
    "                    if depth_slice:\n",
    "                        if (k >= depth_slice_mask[t,j,i]) & (k <= depth_mask[t,j,i]):\n",
    "\n",
    "                            depth_slice_var[t,k,j,i] = variable[k,j,i]\n",
    "\n",
    "                    else:\n",
    "                        if k <= depth_mask[t,j,i]:\n",
    "                            \n",
    "                            new_var[t,k,j,i] = variable[k,j,i]\n",
    "\n",
    "\n",
    "    new_dataset = xr.Dataset(\n",
    "    {\n",
    "        var: (dims, new_var),\n",
    "    }\n",
    "    )\n",
    "\n",
    "    if depth_slice:\n",
    "            slice_dataset = xr.Dataset(\n",
    "    {\n",
    "        var: (dims, depth_slice_var),\n",
    "    }\n",
    "    )\n",
    "    \n",
    "    return slice_dataset if depth_slice else new_dataset\n",
    "\n",
    "def shoc_volume(input_array):\n",
    "\n",
    "    z_diff = np.diff(input_array.z_grid.values)\n",
    "\n",
    "    volume = np.zeros((len(input_array.z_grid)-1,len(input_array.j_centre),len(input_array.i_centre)))\n",
    "\n",
    "    for k in range(len(input_array.z_grid)-2):\n",
    "\n",
    "        volume[k] = input_array.h1acell.values * input_array.h2acell.values * z_diff[k]\n",
    "\n",
    "    volume[len(input_array.z_grid)-2] = input_array.h1acell.values * input_array.h2acell.values * z_diff[len(input_array.z_grid)-3]\n",
    "\n",
    "    return volume\n",
    "\n",
    "def shoc_mass_2(dataset,var,vol):\n",
    "\n",
    "    mass_var = dataset[var] * vol\n",
    "\n",
    "    value_ts = mass_var.sum(dim=['i','j','k'])\n",
    "\n",
    "    return value_ts.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "078dbcdd-2c57-443c-9d82-4f391d0bcaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def longest_period_above_below_threshold(time_series, threshold, above=True):\n",
    "    longest_period = 0\n",
    "    longest_period_start = 0\n",
    "    current_period = 0\n",
    "    current_period_start = 0\n",
    "\n",
    "    comparator = '>' if above else '<'\n",
    "\n",
    "    for i, value in enumerate(time_series):\n",
    "        if eval(f'value {comparator} threshold'):\n",
    "            if current_period == 0:\n",
    "                current_period_start = i\n",
    "            current_period += 1\n",
    "        else:\n",
    "            if current_period > longest_period:\n",
    "                longest_period = current_period\n",
    "                longest_period_start = current_period_start\n",
    "            current_period = 0\n",
    "\n",
    "    if current_period > longest_period:\n",
    "        longest_period = current_period\n",
    "        longest_period_start = current_period_start\n",
    "\n",
    "    return longest_period_start,longest_period\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125b3b38-270c-4669-b24d-a70e09084feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_custom_grid(fig, axes, rows, cols, projection=ccrs.PlateCarree()):\n",
    "    \"\"\"\n",
    "    Apply gridlines to a figure with subplots such that only the leftmost plots\n",
    "    have labels on the left axis, and the bottommost plots have labels on the bottom axis.\n",
    "\n",
    "    Parameters:\n",
    "    fig : matplotlib.figure.Figure\n",
    "        The figure object containing the subplots.\n",
    "    axes : numpy.ndarray\n",
    "        Array of axes objects returned from plt.subplots.\n",
    "    rows : int\n",
    "        Number of rows in the subplot grid.\n",
    "    cols : int\n",
    "        Number of columns in the subplot grid.\n",
    "    projection : cartopy.crs.Projection, optional\n",
    "        The projection for the gridlines. Defaults to PlateCarree.\n",
    "    \"\"\"\n",
    "    \n",
    "    axes = axes.flatten()  # Ensure axes are flattened in case they are a 2D array\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        # Add gridlines with labels\n",
    "        gl = ax.gridlines(crs=projection, draw_labels=True, linewidth=0.5, color='gray', alpha=0.5, linestyle='--')\n",
    "        \n",
    "        # Remove top and right labels for all plots\n",
    "        gl.top_labels = False\n",
    "        gl.right_labels = False\n",
    "        \n",
    "        # Only show left labels for the leftmost plots\n",
    "        if (i % cols) != 0:\n",
    "            gl.left_labels = False\n",
    "\n",
    "        # Only show bottom labels for the bottommost plots\n",
    "        if i < (rows - 1) * cols:\n",
    "            gl.bottom_labels = False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
